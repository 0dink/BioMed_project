{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reader: 1069 | # samples 73\n",
      "reader: 201 | # samples 79\n",
      "reader: 311 | # samples 105\n",
      "reader: 3240 | # samples 85\n",
      "reader: 4297 | # samples 107\n",
      "reader: 7800 | # samples 103\n",
      "reader: 87 | # samples 103\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/extracted_features/mfcc_stats_that_v2/mfcc_features.pickle\", \"rb\") as file:\n",
    "    mfcc_stats_dict = pickle.load(file)\n",
    "\n",
    "for reader in mfcc_stats_dict.keys():\n",
    "    print(f\"reader: {reader} | # samples {len(mfcc_stats_dict[reader])}\")\n",
    "    # for mfcc, file_id in mfcc_stats_dict[reader]:\n",
    "    #     print(f\"\\t 1st 2 mfcc: {mfcc} | # features: {mfcc.shape[0]} | file ID: {file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader | gender\n",
    "# 201    |  man\n",
    "# 311    |  man\n",
    "# 3240   |  man\n",
    "# 87     | woman\n",
    "# 4297   | woman\n",
    "# 7800   | woman\n",
    "# 1069   | woman (low sample size) (deprecated past mfcc_40_features.pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_data(mfcc_dict):\n",
    "    number_of_samples = []\n",
    "    for reader in mfcc_dict.keys():\n",
    "        number_of_samples.append(len(mfcc_dict[reader]))\n",
    "    \n",
    "    return min(number_of_samples)\n",
    "\n",
    "def partition_data(reader, max, split=.8):\n",
    "    mfccs = [mfcc for mfcc, _ in reader]\n",
    "    mfccs = mfccs[0:max]\n",
    "\n",
    "    mfccs_train = mfccs[0:round(len(mfccs)*split)]\n",
    "    mfccs_test  = mfccs[round(len(mfccs)*split):len(mfccs)]\n",
    "\n",
    "    return mfccs_train, mfccs_test\n",
    "\n",
    "def separate_labels(labels, speaker_keys):\n",
    "    separated_labels = []\n",
    "    current_index = 0\n",
    "    for key in speaker_keys:\n",
    "        separated_labels.append(labels[current_index:current_index+key])\n",
    "        current_index += key\n",
    "    return(separated_labels)\n",
    "\n",
    "def calculate_percentage(data):\n",
    "    percentages = []\n",
    "    for sublist in data:\n",
    "        total_count = len(sublist)\n",
    "        if total_count == 0:\n",
    "            percentages.append({})\n",
    "            continue\n",
    "        \n",
    "        count_dict = {}\n",
    "        for num in sublist:\n",
    "            count_dict[num] = count_dict.get(num, 0) + 1\n",
    "        \n",
    "        percentage_dict = {num: (count / total_count) * 100 for num, count in count_dict.items()}\n",
    "        percentages.append(percentage_dict)\n",
    "    \n",
    "    return percentages\n",
    "\n",
    "def format_percentages(percentages):\n",
    "    for i, percentage_dict in enumerate(percentages):\n",
    "        if not percentage_dict:\n",
    "            print(f\"Sublist {i + 1}: No data\")\n",
    "            continue\n",
    "        \n",
    "        # Sort by percentage in descending order\n",
    "        sorted_percentages = sorted(percentage_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Format and print each sublist\n",
    "        formatted_str = f\"Speaker {i + 1}:\\n\\t\" + \", \".join(\n",
    "            f\"{num}: {percent:.2f}%\" for num, percent in sorted_percentages\n",
    "        )\n",
    "        print(formatted_str)\n",
    "\n",
    "def create_truth_list(samples, label_order):\n",
    "    \"\"\"\n",
    "    label_order: \n",
    "        0 for 1st half 0 and 2nd half 1\n",
    "        1 for 1st half 1 and 2nd half 0\n",
    "    \"\"\"\n",
    "    if label_order == 0:\n",
    "        first_half  = [0 for _ in range(0, samples//2)]\n",
    "        second_half = [1 for _ in range(0, samples//2)]        \n",
    "    elif label_order == 1:\n",
    "        first_half  = [1 for _ in range(0, samples//2)]\n",
    "        second_half = [0 for _ in range(0, samples//2)]\n",
    "    else:\n",
    "        print(\"use 0 or 1\")\n",
    "    \n",
    "    first_half.extend(second_half)\n",
    "    return first_half\n",
    "\n",
    "def create_binary_labels(length):\n",
    "    return [(0 if i<length else 1) for i in range(0,2*length)]\n",
    "\n",
    "def generate_confusion_matrix(model_a, model_b, test_a, test_b):\n",
    "    \"\"\"\n",
    "    Generates a confusion matrix for the given HMM models and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - model_a: Trained HMM model for class A\n",
    "    - model_b: Trained HMM model for class B\n",
    "    - test_a: List of test samples for class A\n",
    "    - test_b: List of test samples for class B\n",
    "\n",
    "    Returns:\n",
    "    - [TP, FP\n",
    "       FN, TN].\n",
    "    \"\"\"\n",
    "    # Define the true labels (0 for class A, 1 for class B)\n",
    "    true_labels = [0] * len(test_a) + [1] * len(test_b)\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Predict class for each sample in test_a\n",
    "    for sample in test_a:\n",
    "        log_likelihood_a = model_a.score(sample.reshape(1,-1))\n",
    "        log_likelihood_b = model_b.score(sample.reshape(1,-1))\n",
    "        \n",
    "        predicted_label = 0 if log_likelihood_a > log_likelihood_b else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Predict class for each sample in test_b\n",
    "    for sample in test_b:\n",
    "        log_likelihood_a = model_a.score(sample.reshape(1,-1))\n",
    "        log_likelihood_b = model_b.score(sample.reshape(1,-1))\n",
    "        \n",
    "        predicted_label = 0 if log_likelihood_a > log_likelihood_b else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(features):\n",
    "    model = hmm.GaussianHMM(n_components=5, covariance_type=\"diag\", n_iter=1000,random_state=42)\n",
    "    model.fit(features)\n",
    "    return model\n",
    "\n",
    "def recognize_speaker(models, test_features):\n",
    "    scores = {}\n",
    "    for speaker, model in models.items():\n",
    "        score = model.score(test_features)\n",
    "        scores[speaker] = score\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of samples 87 & 201: 79\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# different gender #\n",
    "# 87 woman 201 man #\n",
    "####################\n",
    "\n",
    "max_num_samples = 79\n",
    "\n",
    "print(f\"max number of samples 87 & 201: {max_num_samples}\")\n",
    "\n",
    "train_87_f, test_87_f = partition_data(mfcc_stats_dict[\"87\"], max_num_samples)\n",
    "train_4297_f, test_4297_f = partition_data(mfcc_stats_dict[\"4297\"], max_num_samples)\n",
    "train_7800_f, test_7800_f = partition_data(mfcc_stats_dict[\"7800\"], max_num_samples)\n",
    "\n",
    "train_201_m, test_201_m = partition_data(mfcc_stats_dict[\"201\"], max_num_samples)\n",
    "train_311_m, test_311_m = partition_data(mfcc_stats_dict[\"311\"], max_num_samples)\n",
    "train_3240_m, test_3240_m = partition_data(mfcc_stats_dict[\"3240\"], max_num_samples)\n",
    "\n",
    "speaker_87_model_f = train_hmm(train_87_f)\n",
    "speaker_4297_model_f = train_hmm(train_4297_f)\n",
    "speaker_7800_model_f = train_hmm(train_7800_f)\n",
    "\n",
    "speaker_201_model_m = train_hmm(train_201_m)\n",
    "speaker_311_model_m = train_hmm(train_311_m)\n",
    "\n",
    "speaker_models = {\"87\": speaker_87_model_f, \"201\": speaker_201_model_m, \"311\": speaker_311_model_m}\n",
    "\n",
    "predicted_speaker_87 = recognize_speaker(speaker_models, test_87)\n",
    "predicted_speaker_201 = recognize_speaker(speaker_models, test_201)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0]\n",
      " [ 2 14]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mat = generate_confusion_matrix(speaker_87_model, speaker_201_model, test_87, test_201)\n",
    "\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[16  0]\n",
    "#  [ 1 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  5]\n",
      " [ 0 16]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mat = generate_confusion_matrix(speaker_311_model, speaker_201_model, test_311, test_201)\n",
    "\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[ 5 11]\n",
    "#  [ 0 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_score(model,test_data):\n",
    "    scores = []\n",
    "    for data_point in test_data:\n",
    "        scores.append(model.score(data_point.reshape(1,-1)))\n",
    "    return scores, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 87 and 87: -596.4729633121453\n",
      "model 87 and 311: -860.6547799319397\n",
      "model 87 and 201: -766.22397302437\n"
     ]
    }
   ],
   "source": [
    "scores, avg_score = average_score(speaker_87_model, test_87)\n",
    "print(f\"model 87 and 87: {avg_score}\")\n",
    "\n",
    "scores, avg_score = average_score(speaker_87_model, test_311)\n",
    "print(f\"model 87 and 311: {avg_score}\")\n",
    "\n",
    "scores, avg_score = average_score(speaker_87_model, test_201)\n",
    "print(f\"model 87 and 201: {avg_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 87 and 87: -441.6667726157509\n",
    "# model 87 and 311: -693.6383601026669\n",
    "# model 87 and 201: -612.5665587149906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 201 and 201: -608.1176310787316\n",
      "model 201 and 311: -666.0221240448473\n",
      "model 201 and 87: -846.0205529979579\n"
     ]
    }
   ],
   "source": [
    "scores, avg_score = average_score(speaker_201_model, test_201)\n",
    "print(f\"model 201 and 201: {avg_score}\")\n",
    "\n",
    "scores, avg_score = average_score(speaker_201_model, test_311)\n",
    "print(f\"model 201 and 311: {avg_score}\")\n",
    "\n",
    "scores, avg_score = average_score(speaker_201_model, test_87)\n",
    "print(f\"model 201 and 87: {avg_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 201 and 201: -435.009762183381\n",
    "# model 201 and 311: -482.7640005389828\n",
    "# model 201 and 87: -634.9915213900033"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioMed_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
