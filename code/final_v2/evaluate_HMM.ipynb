{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from collections import Counter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reader: 1069 | # samples 73\n",
      "reader: 201 | # samples 79\n",
      "reader: 311 | # samples 105\n",
      "reader: 3240 | # samples 85\n",
      "reader: 4297 | # samples 107\n",
      "reader: 7800 | # samples 103\n",
      "reader: 87 | # samples 103\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/extracted_features/mfcc_stats_that_v2/mfcc_40_features.pickle\", \"rb\") as file:\n",
    "    mfcc_stats_dict = pickle.load(file)\n",
    "\n",
    "for reader in mfcc_stats_dict.keys():\n",
    "    print(f\"reader: {reader} | # samples {len(mfcc_stats_dict[reader])}\")\n",
    "    # for mfcc, file_id in mfcc_stats_dict[reader]:\n",
    "    #     print(f\"\\t 1st 2 mfcc: {mfcc} | # features: {mfcc.shape[0]} | file ID: {file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_data(mfcc_dict):\n",
    "    number_of_samples = []\n",
    "    for reader in mfcc_dict.keys():\n",
    "        number_of_samples.append(len(mfcc_dict[reader]))\n",
    "    \n",
    "    return min(number_of_samples)\n",
    "\n",
    "def partition_data(reader, max, split=.8):\n",
    "    mfccs = [mfcc for mfcc, _ in reader]\n",
    "    mfccs = mfccs[0:max]\n",
    "\n",
    "    mfccs_train = mfccs[0:round(len(mfccs)*split)]\n",
    "    mfccs_test  = mfccs[round(len(mfccs)*split):len(mfccs)]\n",
    "\n",
    "    return mfccs_train, mfccs_test\n",
    "\n",
    "def separate_labels(labels, speaker_keys):\n",
    "    separated_labels = []\n",
    "    current_index = 0\n",
    "    for key in speaker_keys:\n",
    "        separated_labels.append(labels[current_index:current_index+key])\n",
    "        current_index += key\n",
    "    return(separated_labels)\n",
    "\n",
    "def calculate_percentage(data):\n",
    "    percentages = []\n",
    "    for sublist in data:\n",
    "        total_count = len(sublist)\n",
    "        if total_count == 0:\n",
    "            percentages.append({})\n",
    "            continue\n",
    "        \n",
    "        count_dict = {}\n",
    "        for num in sublist:\n",
    "            count_dict[num] = count_dict.get(num, 0) + 1\n",
    "        \n",
    "        percentage_dict = {num: (count / total_count) * 100 for num, count in count_dict.items()}\n",
    "        percentages.append(percentage_dict)\n",
    "    \n",
    "    return percentages\n",
    "\n",
    "def format_percentages(percentages):\n",
    "    for i, percentage_dict in enumerate(percentages):\n",
    "        if not percentage_dict:\n",
    "            print(f\"Sublist {i + 1}: No data\")\n",
    "            continue\n",
    "        \n",
    "        # Sort by percentage in descending order\n",
    "        sorted_percentages = sorted(percentage_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Format and print each sublist\n",
    "        formatted_str = f\"Speaker {i + 1}:\\n\\t\" + \", \".join(\n",
    "            f\"{num}: {percent:.2f}%\" for num, percent in sorted_percentages\n",
    "        )\n",
    "        print(formatted_str)\n",
    "\n",
    "def create_truth_list(samples, label_order):\n",
    "    \"\"\"\n",
    "    label_order: \n",
    "        0 for 1st half 0 and 2nd half 1\n",
    "        1 for 1st half 1 and 2nd half 0\n",
    "    \"\"\"\n",
    "    if label_order == 0:\n",
    "        first_half  = [0 for _ in range(0, samples//2)]\n",
    "        second_half = [1 for _ in range(0, samples//2)]        \n",
    "    elif label_order == 1:\n",
    "        first_half  = [1 for _ in range(0, samples//2)]\n",
    "        second_half = [0 for _ in range(0, samples//2)]\n",
    "    else:\n",
    "        print(\"use 0 or 1\")\n",
    "    \n",
    "    first_half.extend(second_half)\n",
    "    return first_half\n",
    "\n",
    "def create_binary_labels(length):\n",
    "    return [(0 if i<length else 1) for i in range(0,2*length)]\n",
    "\n",
    "def generate_confusion_matrix(model_a, model_b, test_a, test_b):\n",
    "    \"\"\"\n",
    "    Generates a confusion matrix for the given HMM models and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - model_a: Trained HMM model for class A\n",
    "    - model_b: Trained HMM model for class B\n",
    "    - test_a: List of test samples for class A\n",
    "    - test_b: List of test samples for class B\n",
    "\n",
    "    Returns:\n",
    "    - [TP, FP\n",
    "       FN, TN].\n",
    "    \"\"\"\n",
    "    # Define the true labels (0 for class A, 1 for class B)\n",
    "    true_labels = [0] * len(test_a) + [1] * len(test_b)\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Predict class for each sample in test_a\n",
    "    for sample in test_a:\n",
    "        log_likelihood_a = model_a.score(sample.reshape(1,-1))\n",
    "        log_likelihood_b = model_b.score(sample.reshape(1,-1))\n",
    "        \n",
    "        predicted_label = 0 if log_likelihood_a > log_likelihood_b else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Predict class for each sample in test_b\n",
    "    for sample in test_b:\n",
    "        log_likelihood_a = model_a.score(sample.reshape(1,-1))\n",
    "        log_likelihood_b = model_b.score(sample.reshape(1,-1))\n",
    "        \n",
    "        predicted_label = 0 if log_likelihood_a > log_likelihood_b else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_score(model,test_data):\n",
    "    scores = []\n",
    "    for data_point in test_data:\n",
    "        scores.append(model.score(data_point.reshape(1,-1)))\n",
    "    return scores, np.mean(scores)\n",
    "\n",
    "def average_score_compare(model_dict, test_data_dict):\n",
    "    for model_key in model_dict.keys():\n",
    "        score_list = []\n",
    "        for data_key in test_data_dict.keys():\n",
    "            _, avg_score = average_score(model_dict[model_key], test_data_dict[data_key])\n",
    "            avg_score = round(float(avg_score), 3)\n",
    "            score_list.append((data_key, avg_score))\n",
    "        print(f\"model {model_key}: {score_list}\")\n",
    "\n",
    "def concatenate_with_key_first(dictionary, key):\n",
    "    # Check if the key exists in the dictionary\n",
    "    if key not in dictionary:\n",
    "        raise KeyError(f\"The key '{key}' does not exist in the dictionary.\")\n",
    "    \n",
    "    # Get the list corresponding to the key\n",
    "    key_list = dictionary[key]\n",
    "    \n",
    "    # Concatenate the key's list with all other lists\n",
    "    result = key_list.copy()  # Start with the list for the specified key\n",
    "    for k, v in dictionary.items():\n",
    "        if k != key:  # Skip the list that corresponds to the key\n",
    "            result.extend(v)  # Add the other lists\n",
    "    \n",
    "    return result, len(key_list)\n",
    "\n",
    "def generate_metrics(model_dict, data_dict, key):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - [TP, FP\n",
    "       FN, TN].\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model_dict[key]\n",
    "    control_data  = data_dict[key]\n",
    "    _, threshold = average_score(model, control_data)\n",
    "\n",
    "    data, segments_length = concatenate_with_key_first(data_dict, key)\n",
    "\n",
    "    ground_truth = [0] * segments_length + [1] * (len(data)-segments_length)\n",
    "    predicted_labels = []\n",
    "    for data_point in data:\n",
    "        score = model.score(data_point.reshape(1,-1))\n",
    "        predicted_label = 0 if score >= threshold else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "    \n",
    "    matrix = confusion_matrix(ground_truth, predicted_labels)\n",
    "    accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "    precision = precision_score(ground_truth, predicted_labels)\n",
    "    recall = recall_score(ground_truth, predicted_labels)\n",
    "    f1 = f1_score(ground_truth, predicted_labels)\n",
    "\n",
    "    return matrix, accuracy, precision, recall, f1\n",
    "\n",
    "def save_metrics(model_dict, data_dict, output_file=\"./metrics.txt\"):\n",
    "    metric_dict = {}\n",
    "    \n",
    "    for key in model_dict.keys():\n",
    "        matrix, accuracy, precision, recall, f1 = generate_metrics(model_dict, data_dict, key)\n",
    "\n",
    "        if isinstance(matrix, np.ndarray):\n",
    "            matrix = matrix.tolist()\n",
    "\n",
    "        metric_dict[key] = {\n",
    "            'matrix': matrix,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for key, metrics in metric_dict.items():\n",
    "            f.write(f\"{key}:\\n\")\n",
    "            f.write(f\"    accuracy: {round(metrics['accuracy'], 4)}\\n\")\n",
    "            f.write(f\"    precision: {round(metrics['precision'], 4)}\\n\")\n",
    "            f.write(f\"    recall: {round(metrics['recall'], 4)}\\n\")\n",
    "            f.write(f\"    F1-score: {round(metrics['f1'], 4)}\\n\")\n",
    "            \n",
    "            # Formatting the matrix\n",
    "            f.write(f\"    matrix:\\n\")\n",
    "            for row in metrics['matrix']:\n",
    "                f.write(f\"        {row}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    return metric_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(features):\n",
    "    model = hmm.GaussianHMM(n_components=5, covariance_type=\"diag\", n_iter=1000,random_state=55)\n",
    "    model.fit(features)\n",
    "    return model\n",
    "\n",
    "def recognize_speaker(models, test_features):\n",
    "    scores = {}\n",
    "    for speaker, model in models.items():\n",
    "        score = model.score(test_features)\n",
    "        scores[speaker] = score\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of samples 87 & 201: 79\n"
     ]
    }
   ],
   "source": [
    "max_num_samples = 79\n",
    "\n",
    "print(f\"max number of samples 87 & 201: {max_num_samples}\")\n",
    "\n",
    "train_87_f, test_87_f = partition_data(mfcc_stats_dict[\"87\"], max_num_samples)\n",
    "train_4297_f, test_4297_f = partition_data(mfcc_stats_dict[\"4297\"], max_num_samples)\n",
    "train_7800_f, test_7800_f = partition_data(mfcc_stats_dict[\"7800\"], max_num_samples)\n",
    "\n",
    "train_201_m, test_201_m = partition_data(mfcc_stats_dict[\"201\"], max_num_samples)\n",
    "train_311_m, test_311_m = partition_data(mfcc_stats_dict[\"311\"], max_num_samples)\n",
    "train_3240_m, test_3240_m = partition_data(mfcc_stats_dict[\"3240\"], max_num_samples)\n",
    "\n",
    "speaker_87_model_f = train_hmm(train_87_f)\n",
    "speaker_4297_model_f = train_hmm(train_4297_f)\n",
    "speaker_7800_model_f = train_hmm(train_7800_f)\n",
    "\n",
    "speaker_201_model_m = train_hmm(train_201_m)\n",
    "speaker_311_model_m = train_hmm(train_311_m)\n",
    "speaker_3240_model_m = train_hmm(train_3240_m)\n",
    "\n",
    "speaker_models = {\"87_f\"   : speaker_87_model_f,\n",
    "                  \"4297_f\" : speaker_4297_model_f,\n",
    "                  \"7800_f\" : speaker_7800_model_f,\n",
    "                  \"201_m\"  : speaker_201_model_m,\n",
    "                  \"311_m\"  : speaker_311_model_m,\n",
    "                  \"3240_m\" : speaker_3240_model_m\n",
    "                  }\n",
    "\n",
    "test_data = {\"87_f\"   : test_87_f,\n",
    "             \"4297_f\" : test_4297_f,\n",
    "             \"7800_f\" : test_7800_f,\n",
    "             \"201_m\"  : test_201_m,\n",
    "             \"311_m\"  : test_311_m,\n",
    "             \"3240_m\" : test_3240_m\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87_f': {'matrix': [[10, 6], [24, 56]],\n",
       "  'accuracy': 0.6875,\n",
       "  'precision': np.float64(0.9032258064516129),\n",
       "  'recall': np.float64(0.7),\n",
       "  'f1': np.float64(0.7887323943661971)},\n",
       " '4297_f': {'matrix': [[9, 7], [6, 74]],\n",
       "  'accuracy': 0.8645833333333334,\n",
       "  'precision': np.float64(0.9135802469135802),\n",
       "  'recall': np.float64(0.925),\n",
       "  'f1': np.float64(0.9192546583850931)},\n",
       " '7800_f': {'matrix': [[10, 6], [3, 77]],\n",
       "  'accuracy': 0.90625,\n",
       "  'precision': np.float64(0.927710843373494),\n",
       "  'recall': np.float64(0.9625),\n",
       "  'f1': np.float64(0.9447852760736196)},\n",
       " '201_m': {'matrix': [[10, 6], [1, 79]],\n",
       "  'accuracy': 0.9270833333333334,\n",
       "  'precision': np.float64(0.9294117647058824),\n",
       "  'recall': np.float64(0.9875),\n",
       "  'f1': np.float64(0.9575757575757575)},\n",
       " '311_m': {'matrix': [[7, 9], [6, 74]],\n",
       "  'accuracy': 0.84375,\n",
       "  'precision': np.float64(0.891566265060241),\n",
       "  'recall': np.float64(0.925),\n",
       "  'f1': np.float64(0.9079754601226994)},\n",
       " '3240_m': {'matrix': [[10, 6], [8, 72]],\n",
       "  'accuracy': 0.8541666666666666,\n",
       "  'precision': np.float64(0.9230769230769231),\n",
       "  'recall': np.float64(0.9),\n",
       "  'f1': np.float64(0.9113924050632911)}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_metrics(speaker_models, test_data, output_file=\"./metrics/mfcc_40.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "accuracy : 0.8541666666666666\n",
      "precision: 0.9125\n",
      "recall   : 0.9125\n",
      "F1-score : 0.9125\n",
      "[[ 9  7]\n",
      " [ 7 73]]\n"
     ]
    }
   ],
   "source": [
    "# matrix, accuracy, precision, recall, f1 = generate_metrics(speaker_models, test_data, \"87_f\")\n",
    "# print(f\"accuracy : {accuracy}\")\n",
    "# print(f\"precision: {precision}\")\n",
    "# print(f\"recall   : {recall}\")\n",
    "# print(f\"F1-score : {f1}\")\n",
    "# print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 87_f: [('87_f', -410.449), ('4297_f', -604.492), ('7800_f', -552.027), ('201_m', -537.214), ('311_m', -570.359), ('3240_m', -479.346)]\n",
      "model 4297_f: [('87_f', -391.431), ('4297_f', -483.063), ('7800_f', -484.485), ('201_m', -440.781), ('311_m', -461.363), ('3240_m', -432.807)]\n",
      "model 7800_f: [('87_f', -197092661.083), ('4297_f', -19742027.579), ('7800_f', -381915028.442), ('201_m', -19784837.953), ('311_m', -5398641.518), ('3240_m', -37067899.481)]\n",
      "model 201_m: [('87_f', -527.532), ('4297_f', -423.004), ('7800_f', -480.147), ('201_m', -399.365), ('311_m', -368.263), ('3240_m', -402.237)]\n",
      "model 311_m: [('87_f', -457.635), ('4297_f', -427.259), ('7800_f', -545.359), ('201_m', -402.126), ('311_m', -372.316), ('3240_m', -392.413)]\n",
      "model 3240_m: [('87_f', -437.078), ('4297_f', -465.715), ('7800_f', -477.863), ('201_m', -464.916), ('311_m', -399.723), ('3240_m', -393.203)]\n"
     ]
    }
   ],
   "source": [
    "average_score_compare(speaker_models, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 87_f: [('87_f', -441.168), ('4297_f', -560.676), ('7800_f', -545.1), ('201_m', -542.829), ('311_m', -618.503), ('3240_m', -535.57)]\n",
    "# model 4297_f: [('87_f', -688.566), ('4297_f', -616.299), ('7800_f', -880.394), ('201_m', -840.099), ('311_m', -725.657), ('3240_m', -786.03)]\n",
    "# model 7800_f: [('87_f', -518.996), ('4297_f', -565.683), ('7800_f', -461.715), ('201_m', -698.191), ('311_m', -718.864), ('3240_m', -647.898)]\n",
    "# model 201_m: [('87_f', -566.325), ('4297_f', -495.91), ('7800_f', -644.147), ('201_m', -433.686), ('311_m', -447.226), ('3240_m', -477.597)]\n",
    "# model 311_m: [('87_f', -737.568), ('4297_f', -593.413), ('7800_f', -817.242), ('201_m', -545.303), ('311_m', -466.979), ('3240_m', -575.137)]\n",
    "# model 3240_m: [('87_f', -501.412), ('4297_f', -481.066), ('7800_f', -554.355), ('201_m', -473.8), ('311_m', -457.222), ('3240_m', -412.08)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioMed_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
