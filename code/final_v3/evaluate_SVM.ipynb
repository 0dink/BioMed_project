{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reader: 1069 | # samples 300\n",
      "reader: 19 | # samples 300\n",
      "reader: 201 | # samples 300\n",
      "reader: 250 | # samples 300\n",
      "reader: 254 | # samples 300\n",
      "reader: 26 | # samples 300\n",
      "reader: 27 | # samples 300\n",
      "reader: 289 | # samples 300\n",
      "reader: 298 | # samples 300\n",
      "reader: 311 | # samples 300\n",
      "reader: 32 | # samples 300\n",
      "reader: 3240 | # samples 300\n",
      "reader: 39 | # samples 300\n",
      "reader: 40 | # samples 300\n",
      "reader: 4297 | # samples 300\n",
      "reader: 60 | # samples 300\n",
      "reader: 78 | # samples 300\n",
      "reader: 7800 | # samples 300\n",
      "reader: 83 | # samples 300\n",
      "reader: 87 | # samples 300\n"
     ]
    }
   ],
   "source": [
    "features = \"mfcc_13_features\"\n",
    "\n",
    "with open(f\"../../data/extracted_features_v2/{features}.pickle\", \"rb\") as file:\n",
    "   mfcc_stats_dict = pickle.load(file)\n",
    "\n",
    "for reader in mfcc_stats_dict.keys():\n",
    "    print(f\"reader: {reader} | # samples {len(mfcc_stats_dict[reader])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(reader, max, split=.8):\n",
    "    mfccs = [mfcc for mfcc in reader]\n",
    "    mfccs = mfccs[0:max]\n",
    "\n",
    "    mfccs_train = mfccs[0:round(len(mfccs)*split)]\n",
    "    mfccs_test  = mfccs[round(len(mfccs)*split):len(mfccs)]\n",
    "\n",
    "    return mfccs_train, mfccs_test\n",
    "\n",
    "def partition_wrapper(data_dictionary, max):\n",
    "    test_dict = {}\n",
    "    train_dict = {}\n",
    "\n",
    "    for key in data_dictionary.keys():\n",
    "        train, test = partition_data(data_dictionary[key], max)\n",
    "        train_dict[key] = train\n",
    "        test_dict[key] = test\n",
    "\n",
    "    return train_dict, test_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_indices(n, x, seed=42):\n",
    "    if n > x + 1:\n",
    "        raise ValueError(\"Cannot generate more unique numbers than the specified range.\")\n",
    "    \n",
    "    random.seed(seed)  # Set the seed for reproducibility\n",
    "    return random.sample(range(0, x), n)\n",
    "\n",
    "def average_score(model,test_data):\n",
    "    scores = []\n",
    "    for data_point in test_data:\n",
    "        scores.append(model.score(data_point.reshape(1,-1)))\n",
    "    return scores, np.mean(scores)\n",
    "\n",
    "def average_score_compare(model_dict, test_data_dict):\n",
    "    for model_key in model_dict.keys():\n",
    "        score_list = []\n",
    "        for data_key in test_data_dict.keys():\n",
    "            _, avg_score = average_score(model_dict[model_key], test_data_dict[data_key])\n",
    "            avg_score = round(float(avg_score), 3)\n",
    "            score_list.append((data_key, avg_score))\n",
    "        print(f\"model {model_key}: {score_list}\")\n",
    "\n",
    "def generate_binary_test_set(data_dict, key):\n",
    "    if key not in data_dict:\n",
    "        raise KeyError(f\"The key '{key}' does not exist in the dictionary.\")\n",
    "    \n",
    "    true_values = data_dict[key] # Get the list corresponding to the key\n",
    "\n",
    "    num_other_classes = len(data_dict.keys()) - 1\n",
    "    num_of_true_samples = len(true_values)\n",
    "\n",
    "    samples_per_class = num_of_true_samples // num_other_classes\n",
    "    # print(f\"samples per class: {samples_per_class}\")\n",
    "\n",
    "    random_indices = generate_random_indices(samples_per_class, num_of_true_samples)\n",
    "\n",
    "    test_set = true_values.copy()  # Start with the list for the specified key\n",
    "    \n",
    "    for k, v in data_dict.items():\n",
    "        if k != key:  # Skip the list that corresponds to the key\n",
    "            for i in random_indices:\n",
    "                test_set.append(v[i])\n",
    "\n",
    "    return test_set, num_of_true_samples\n",
    "\n",
    "def generate_metrics(model_dict, data_dict, key):\n",
    "   \"\"\"\n",
    "   Returns metrics for One-Class SVM using predict():\n",
    "   - [TP, FP\n",
    "      FN, TN].\n",
    "   \"\"\"\n",
    "   \n",
    "   model = model_dict[key]\n",
    "   control_data = data_dict[key]\n",
    "\n",
    "   # Concatenate data (assuming the data_dict is structured similarly to the original code)\n",
    "   data, segments_length = generate_binary_test_set(data_dict, key)\n",
    "\n",
    "   # Ground truth: first part is non-target (0), second part is target (1)\n",
    "   ground_truth = [0] * segments_length + [1] * (len(data) - segments_length)\n",
    "   \n",
    "   # Get binary predictions (1 for inliers, -1 for outliers)\n",
    "   predicted_labels = model.predict(data)\n",
    "\n",
    "   # Convert predictions to binary labels: 1 for inliers, 0 for outliers\n",
    "   predicted_labels = [1 if label == 1 else 0 for label in predicted_labels]\n",
    "\n",
    "   # Generate confusion matrix and other metrics\n",
    "   matrix = confusion_matrix(ground_truth, predicted_labels)\n",
    "   accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "   precision = precision_score(ground_truth, predicted_labels)\n",
    "   recall = recall_score(ground_truth, predicted_labels)\n",
    "   f1 = f1_score(ground_truth, predicted_labels)\n",
    "   \n",
    "   # For ROC AUC, we use the decision function to get the scores\n",
    "   decision_scores = model.decision_function(data)\n",
    "   roc_auc = roc_auc_score(ground_truth, decision_scores)\n",
    "   fpr, tpr, _ = roc_curve(ground_truth, decision_scores)\n",
    "   \n",
    "   return matrix, accuracy, precision, recall, f1, roc_auc, fpr, tpr\n",
    "\n",
    "def save_metrics(model_dict, data_dict, output_file=\"./metrics.txt\"):\n",
    "    metric_dict = {}\n",
    "    \n",
    "    for key in model_dict.keys():\n",
    "        # threshold, matrix, accuracy, precision, recall, f1, roc_auc, _, _ = generate_metrics(model_dict, data_dict, key)\n",
    "        matrix, accuracy, precision, recall, f1, roc_auc, _, _ = generate_metrics(model_dict, data_dict, key)\n",
    "\n",
    "        if isinstance(matrix, np.ndarray):\n",
    "            matrix = matrix.tolist()\n",
    "\n",
    "        metric_dict[key] = {\n",
    "            #'threshold' : threshold,\n",
    "            'matrix': matrix,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for key, metrics in metric_dict.items():\n",
    "            f.write(f\"{key}:\\n\")\n",
    "            #f.write(f\"    threshold: {round(metrics['threshold'], 4)}\\n\")\n",
    "            f.write(f\"    accuracy: {round(metrics['accuracy'], 4)}\\n\")\n",
    "            f.write(f\"    precision: {round(metrics['precision'], 4)}\\n\")\n",
    "            f.write(f\"    recall: {round(metrics['recall'], 4)}\\n\")\n",
    "            f.write(f\"    F1-score: {round(metrics['f1'], 4)}\\n\")\n",
    "            f.write(f\"    ROC AUC: {round(metrics['roc_auc'], 4)}\\n\")\n",
    "            \n",
    "            # Formatting the matrix\n",
    "            f.write(f\"    matrix:\\n\")\n",
    "            for row in metrics['matrix']:\n",
    "                f.write(f\"        {row}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "def plot_roc_all(model_dict,d ata_dict, features_used=\"\", save_dir=None):\n",
    "    plt.figure(figsize=(13, 11))\n",
    "\n",
    "    for key in model_dict.keys():\n",
    "        _, _, _, _, _, _, roc_auc, fpr, tpr, = generate_metrics(model_dict, data_dict, key)\n",
    "        plt.plot(fpr, tpr, label=f'{key} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random chance\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for All Models {features_used}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    if save_dir:    \n",
    "        plt.savefig(os.path.join(save_dir, f\"{features_used}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()  # Close plot to prevent overlap in successive calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_dict, key):\n",
    "    training_data, segments_length = generate_binary_test_set(train_dict, key)\n",
    "    training_data = np.vstack(training_data)\n",
    "    ground_truth = [0] * segments_length + [1] * (len(training_data) - segments_length)\n",
    "    model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "    return(model.fit(training_data, ground_truth))\n",
    "\n",
    "def train_wrapper(train_dict):\n",
    "    model_dict = {}\n",
    "\n",
    "    for key in train_dict.keys():\n",
    "        model_dict[key] = train_svm(train_dict, key)\n",
    "    \n",
    "    return(model_dict)\n",
    "\n",
    "def train_one_class_svm(train_data):\n",
    "    model = OneClassSVM(kernel=\"rbf\", nu=0.1, gamma=\"auto\")\n",
    "    return model.fit(train_data)\n",
    "\n",
    "def train_one_class_svm_wrapper(train_dict):\n",
    "    model_dict = {}\n",
    "\n",
    "    for key in train_dict.keys():\n",
    "        model_dict[key] = train_svm(train_dict, key)\n",
    "    \n",
    "    return(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = partition_wrapper(mfcc_stats_dict, 300)\n",
    "speaker_models = train_wrapper(train_data)\n",
    "speaker_models_one_class = train_one_class_svm_wrapper(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(speaker_models, test_data, output_file=f\"./metrics/SVM/{features}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1069': {'matrix': [[60, 0], [6, 51]],\n",
       "  'accuracy': 0.9487179487179487,\n",
       "  'precision': np.float64(1.0),\n",
       "  'recall': np.float64(0.8947368421052632),\n",
       "  'f1': np.float64(0.9444444444444444),\n",
       "  'roc_auc': np.float64(0.995906432748538)},\n",
       " '19': {'matrix': [[59, 1], [2, 55]],\n",
       "  'accuracy': 0.9743589743589743,\n",
       "  'precision': np.float64(0.9821428571428571),\n",
       "  'recall': np.float64(0.9649122807017544),\n",
       "  'f1': np.float64(0.9734513274336283),\n",
       "  'roc_auc': np.float64(0.9935672514619883)},\n",
       " '201': {'matrix': [[56, 4], [17, 40]],\n",
       "  'accuracy': 0.8205128205128205,\n",
       "  'precision': np.float64(0.9090909090909091),\n",
       "  'recall': np.float64(0.7017543859649122),\n",
       "  'f1': np.float64(0.7920792079207921),\n",
       "  'roc_auc': np.float64(0.9178362573099416)},\n",
       " '250': {'matrix': [[39, 21], [14, 43]],\n",
       "  'accuracy': 0.7008547008547008,\n",
       "  'precision': np.float64(0.671875),\n",
       "  'recall': np.float64(0.7543859649122807),\n",
       "  'f1': np.float64(0.7107438016528925),\n",
       "  'roc_auc': np.float64(0.7666666666666666)},\n",
       " '254': {'matrix': [[31, 29], [2, 55]],\n",
       "  'accuracy': 0.7350427350427351,\n",
       "  'precision': np.float64(0.6547619047619048),\n",
       "  'recall': np.float64(0.9649122807017544),\n",
       "  'f1': np.float64(0.7801418439716312),\n",
       "  'roc_auc': np.float64(0.9043859649122806)},\n",
       " '26': {'matrix': [[57, 3], [6, 51]],\n",
       "  'accuracy': 0.9230769230769231,\n",
       "  'precision': np.float64(0.9444444444444444),\n",
       "  'recall': np.float64(0.8947368421052632),\n",
       "  'f1': np.float64(0.918918918918919),\n",
       "  'roc_auc': np.float64(0.9815789473684211)},\n",
       " '27': {'matrix': [[59, 1], [19, 38]],\n",
       "  'accuracy': 0.8290598290598291,\n",
       "  'precision': np.float64(0.9743589743589743),\n",
       "  'recall': np.float64(0.6666666666666666),\n",
       "  'f1': np.float64(0.7916666666666666),\n",
       "  'roc_auc': np.float64(0.8894736842105262)},\n",
       " '289': {'matrix': [[58, 2], [4, 53]],\n",
       "  'accuracy': 0.9487179487179487,\n",
       "  'precision': np.float64(0.9636363636363636),\n",
       "  'recall': np.float64(0.9298245614035088),\n",
       "  'f1': np.float64(0.9464285714285714),\n",
       "  'roc_auc': np.float64(0.9915204678362574)},\n",
       " '298': {'matrix': [[56, 4], [13, 44]],\n",
       "  'accuracy': 0.8547008547008547,\n",
       "  'precision': np.float64(0.9166666666666666),\n",
       "  'recall': np.float64(0.7719298245614035),\n",
       "  'f1': np.float64(0.8380952380952381),\n",
       "  'roc_auc': np.float64(0.9678362573099416)},\n",
       " '311': {'matrix': [[52, 8], [12, 45]],\n",
       "  'accuracy': 0.8290598290598291,\n",
       "  'precision': np.float64(0.8490566037735849),\n",
       "  'recall': np.float64(0.7894736842105263),\n",
       "  'f1': np.float64(0.8181818181818182),\n",
       "  'roc_auc': np.float64(0.931578947368421)},\n",
       " '32': {'matrix': [[58, 2], [13, 44]],\n",
       "  'accuracy': 0.8717948717948718,\n",
       "  'precision': np.float64(0.9565217391304348),\n",
       "  'recall': np.float64(0.7719298245614035),\n",
       "  'f1': np.float64(0.8543689320388349),\n",
       "  'roc_auc': np.float64(0.9160818713450293)},\n",
       " '3240': {'matrix': [[58, 2], [9, 48]],\n",
       "  'accuracy': 0.905982905982906,\n",
       "  'precision': np.float64(0.96),\n",
       "  'recall': np.float64(0.8421052631578947),\n",
       "  'f1': np.float64(0.897196261682243),\n",
       "  'roc_auc': np.float64(0.9558479532163743)},\n",
       " '39': {'matrix': [[55, 5], [10, 47]],\n",
       "  'accuracy': 0.8717948717948718,\n",
       "  'precision': np.float64(0.9038461538461539),\n",
       "  'recall': np.float64(0.8245614035087719),\n",
       "  'f1': np.float64(0.8623853211009175),\n",
       "  'roc_auc': np.float64(0.9312865497076023)},\n",
       " '40': {'matrix': [[55, 5], [10, 47]],\n",
       "  'accuracy': 0.8717948717948718,\n",
       "  'precision': np.float64(0.9038461538461539),\n",
       "  'recall': np.float64(0.8245614035087719),\n",
       "  'f1': np.float64(0.8623853211009175),\n",
       "  'roc_auc': np.float64(0.9681286549707603)},\n",
       " '4297': {'matrix': [[57, 3], [11, 46]],\n",
       "  'accuracy': 0.8803418803418803,\n",
       "  'precision': np.float64(0.9387755102040817),\n",
       "  'recall': np.float64(0.8070175438596491),\n",
       "  'f1': np.float64(0.8679245283018868),\n",
       "  'roc_auc': np.float64(0.9529239766081872)},\n",
       " '60': {'matrix': [[57, 3], [10, 47]],\n",
       "  'accuracy': 0.8888888888888888,\n",
       "  'precision': np.float64(0.94),\n",
       "  'recall': np.float64(0.8245614035087719),\n",
       "  'f1': np.float64(0.8785046728971962),\n",
       "  'roc_auc': np.float64(0.9853801169590644)},\n",
       " '78': {'matrix': [[60, 0], [23, 34]],\n",
       "  'accuracy': 0.8034188034188035,\n",
       "  'precision': np.float64(1.0),\n",
       "  'recall': np.float64(0.5964912280701754),\n",
       "  'f1': np.float64(0.7472527472527473),\n",
       "  'roc_auc': np.float64(0.9207602339181286)},\n",
       " '7800': {'matrix': [[60, 0], [7, 50]],\n",
       "  'accuracy': 0.9401709401709402,\n",
       "  'precision': np.float64(1.0),\n",
       "  'recall': np.float64(0.8771929824561403),\n",
       "  'f1': np.float64(0.9345794392523364),\n",
       "  'roc_auc': np.float64(0.9985380116959064)},\n",
       " '83': {'matrix': [[58, 2], [9, 48]],\n",
       "  'accuracy': 0.905982905982906,\n",
       "  'precision': np.float64(0.96),\n",
       "  'recall': np.float64(0.8421052631578947),\n",
       "  'f1': np.float64(0.897196261682243),\n",
       "  'roc_auc': np.float64(0.9730994152046784)},\n",
       " '87': {'matrix': [[56, 4], [15, 42]],\n",
       "  'accuracy': 0.8376068376068376,\n",
       "  'precision': np.float64(0.9130434782608695),\n",
       "  'recall': np.float64(0.7368421052631579),\n",
       "  'f1': np.float64(0.8155339805825242),\n",
       "  'roc_auc': np.float64(0.9637426900584795)}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_metrics(speaker_models_one_class, test_data, output_file=f\"./metrics/SVM/{features}_oneclass.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioMed_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
